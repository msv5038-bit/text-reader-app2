<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Text and Object Reader for Low Vision</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            text-align: center;
            background-color: #f0f0f0;
            margin: 0;
            padding: 20px;
        }
        #video {
            width: 100%;
            max-width: 640px;
            border: 2px solid #333;
            margin-bottom: 20px;
            touch-action: pinch-zoom;
        }
        #canvas {
            display: none;
        }
        button {
            font-size: 1.2em;
            padding: 10px 20px;
            margin: 10px;
            background-color: #007bff;
            color: white;
            border: none;
            border-radius: 5px;
            cursor: pointer;
        }
        button:hover {
            background-color: #0056b3;
        }
        #stopSpeech {
            background-color: #dc3545;
        }
        #stopSpeech:hover {
            background-color: #b02a37;
        }
        #detectObject {
            background-color: #28a745;
        }
        #detectObject:hover {
            background-color: #218838;
        }
        #output {
            font-size: 1.5em;
            margin-top: 20px;
            padding: 20px;
            background-color: white;
            border: 1px solid #ccc;
            border-radius: 5px;
            max-width: 800px;
            margin: 20px auto;
            white-space: pre-wrap;
        }
        body.high-contrast {
            background-color: black;
            color: yellow;
        }
        button.high-contrast {
            background-color: yellow;
            color: black;
        }
        #stopSpeech.high-contrast {
            background-color: #ff5555;
            color: black;
        }
        #detectObject.high-contrast {
            background-color: #55ff55;
            color: black;
        }
    </style>
    <script src="https://cdn.jsdelivr.net/npm/tesseract.js@5/dist/tesseract.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.8.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/mobilenet@2.1.0/dist/mobilenet.min.js"></script>
</head>
<body>
    <h1>Text and Object Reader App</h1>
    <p>Point the camera at printed text and press "Scan and Read" or at an object and press "Detect Object". Ensure good lighting.</p>
    
    <video id="video" autoplay playsinline></video>
    <canvas id="canvas"></canvas>
    
    <label for="textType">Text Type:</label>
    <select id="textType">
        <option value="3">Book/Printed</option>
        <option value="6">Single Block (e.g., Label)</option>
    </select>
    
    <button id="startCamera">Start Camera</button>
    <button id="scan">Scan and Read Text</button>
    <button id="detectObject">Detect Object</button>
    <button id="stopSpeech">Stop Reading</button>
    <button id="toggleFlash">Turn On Flashlight</button>
    <button id="toggleContrast">Toggle High Contrast</button>
    
    <p id="audioWarning" style="color: red;">No sound? Check silent mode, volume, or Bluetooth settings.</p>
    <div id="output">Results will appear here...</div>

    <script>
        const video = document.getElementById('video');
        const canvas = document.getElementById('canvas');
        const output = document.getElementById('output');
        const startCameraBtn = document.getElementById('startCamera');
        const scanBtn = document.getElementById('scan');
        const detectObjectBtn = document.getElementById('detectObject');
        const stopSpeechBtn = document.getElementById('stopSpeech');
        const toggleFlashBtn = document.getElementById('toggleFlash');
        const toggleContrastBtn = document.getElementById('toggleContrast');
        let stream;
        let flashOn = false;
        let highContrast = false;
        let mobilenetModel;

        // Load MobileNet model on page load
        async function loadModel() {
            output.textContent = 'Loading object detection model...';
            try {
                mobilenetModel = await mobilenet.load();
                output.textContent = 'Model loaded. Ready to detect objects.';
            } catch (err) {
                output.textContent = 'Error loading model: ' + err.message;
            }
        }
        loadModel();

        // Start camera with higher resolution
        startCameraBtn.addEventListener('click', async () => {
            try {
                stream = await navigator.mediaDevices.getUserMedia({
                    video: {
                        facingMode: 'environment',
                        width: { ideal: 1280 },
                        height: { ideal: 720 }
                    }
                });
                video.srcObject = stream;
                startCameraBtn.disabled = true;
                scanBtn.disabled = false;
                detectObjectBtn.disabled = false;
                stopSpeechBtn.disabled = false;
            } catch (err) {
                alert('Error accessing camera: ' + err.message);
            }
        });

        // Toggle flashlight
        toggleFlashBtn.addEventListener('click', async () => {
            if (!stream) return;
            const track = stream.getVideoTracks()[0];
            flashOn = !flashOn;
            try {
                await track.applyConstraints({ advanced: [{ torch: flashOn }] });
                toggleFlashBtn.textContent = flashOn ? 'Turn Off Flashlight' : 'Turn On Flashlight';
            } catch (err) {
                alert('Flashlight not supported: ' + err.message);
            }
        });

        // Scan and read text
        scanBtn.addEventListener('click', async () => {
            if (!video.srcObject) {
                alert('Camera not started!');
                return;
            }

            output.textContent = 'Hold steady for clear focus...';
            setTimeout(async () => {
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
                const ctx = canvas.getContext('2d');
                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

                // Preprocess image for OCR
                const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
                const data = imageData.data;
                for (let i = 0; i < data.length; i += 4) {
                    const avg = (data[i] + data[i + 1] + data[i + 2]) / 3;
                    data[i] = data[i + 1] = data[i + 2] = avg > 128 ? 255 : 0;
                }
                ctx.putImageData(imageData, 0, 0);
                const imageDataURL = canvas.toDataURL('image/png');

                // OCR with Tesseract.js
                output.textContent = 'Scanning text...';
                try {
                    const psmMode = document.getElementById('textType').value;
                    const { data: { text } } = await Tesseract.recognize(imageDataURL, 'eng', {
                        logger: m => console.log(m),
                        tessedit_pageseg_mode: psmMode,
                        tessedit_char_whitelist: 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789.,!?- '
                    });
                    output.textContent = text || 'No text detected.';

                    // Speak text
                    if ('speechSynthesis' in window) {
                        speechSynthesis.cancel();
                        const utterance = new SpeechSynthesisUtterance(text || 'No text detected.');
                        utterance.volume = 1;
                        utterance.rate = 1;
                        utterance.pitch = 1;
                        const voices = speechSynthesis.getVoices();
                        utterance.voice = voices.find(voice => voice.lang === 'en-US') || voices[0];
                        utterance.onerror = (event) => {
                            output.textContent = 'Speech error: ' + event.error;
                        };
                        utterance.onend = () => {
                            output.textContent = text || 'No text detected.';
                        };
                        speechSynthesis.speak(utterance);
                    } else {
                        output.textContent = 'Speech synthesis not supported on this device.';
                    }
                } catch (err) {
                    output.textContent = 'Error during OCR: ' + err.message;
                }
            }, 1000);
        });

        // Detect object
        detectObjectBtn.addEventListener('click', async () => {
            if (!video.srcObject) {
                alert('Camera not started!');
                return;
            }
            if (!mobilenetModel) {
                alert('Object detection model not loaded yet!');
                return;
            }

            output.textContent = 'Hold steady for object detection...';
            setTimeout(async () => {
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
                const ctx = canvas.getContext('2d');
                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

                // Use video frame directly for MobileNet
                try {
                    const predictions = await mobilenetModel.classify(video, 1); // Get top prediction
                    const objectName = predictions[0]?.className || 'No object detected';
                    const confidence = predictions[0]?.probability?.toFixed(2) || 0;
                    output.textContent = `Object: ${objectName} (Confidence: ${confidence})`;

                    // Speak object name
                    if ('speechSynthesis' in window) {
                        speechSynthesis.cancel();
                        const utterance = new SpeechSynthesisUtterance(`Object detected: ${objectName}`);
                        utterance.volume = 1;
                        utterance.rate = 1;
                        utterance.pitch = 1;
                        const voices = speechSynthesis.getVoices();
                        utterance.voice = voices.find(voice => voice.lang === 'en-US') || voices[0];
                        utterance.onerror = (event) => {
                            output.textContent = 'Speech error: ' + event.error;
                        };
                        utterance.onend = () => {
                            output.textContent = `Object: ${objectName} (Confidence: ${confidence})`;
                        };
                        speechSynthesis.speak(utterance);
                    } else {
                        output.textContent = 'Speech synthesis not supported on this device.';
                    }
                } catch (err) {
                    output.textContent = 'Error during object detection: ' + err.message;
                }
            }, 1000);
        });

        // Stop speech
        stopSpeechBtn.addEventListener('click', () => {
            if ('speechSynthesis' in window) {
                speechSynthesis.cancel();
                output.textContent = 'Speech stopped.';
            }
        });

        // Toggle high contrast mode
        toggleContrastBtn.addEventListener('click', () => {
            highContrast = !highContrast;
            document.body.classList.toggle('high-contrast', highContrast);
            toggleContrastBtn.classList.toggle('high-contrast', highContrast);
            scanBtn.classList.toggle('high-contrast', highContrast);
            startCameraBtn.classList.toggle('high-contrast', highContrast);
            toggleFlashBtn.classList.toggle('high-contrast', highContrast);
            stopSpeechBtn.classList.toggle('high-contrast', highContrast);
            detectObjectBtn.classList.toggle('high-contrast', highContrast);
        });

        // Ensure voices are loaded
        window.speechSynthesis.onvoiceschanged = () => {
            console.log('Voices loaded:', speechSynthesis.getVoices());
        };

        // Stop stream and speech on page unload
        window.addEventListener('beforeunload', () => {
            if (stream) {
                stream.getTracks().forEach(track => track.stop());
            }
            speechSynthesis.cancel();
        });
    </script>
</body>
</html>
